class MyCustomDataset(Dataset):   
    def __init__(self, data, root_dir, transform=None, target_transform=None):
        self.img_labels = pd.read_csv(data)
        self.root_dir = root_dir
        self.transform = transform
        self.target_transform = target_transform
    
    def __len__(self):
        return len(self.img_labels)
    
    def __getitem__(self, idx):
        #img_path = os.path.join(self.root_dir, self.img_labels.iloc[idx, 0])
        img_path = self.img_labels.iloc[idx, 2]
        image = Image.open(img_path)
        label = self.img_labels.iloc[idx, 3]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            image = self.target_transform(image)
        return image, label


from torchvision.transforms import v2

transforms = v2.Compose([
    v2.Resize(size=(256, 306)), # Bild skalieren
    v2.CenterCrop(size=256), # Seiten beschneiden   
    v2.RandomHorizontalFlip(p=0.5), # horizontal spiegeln
    v2.RandomVerticalFlip(p=0.5),  # vertikal spiegeln
    v2.RandomRotation(degrees=(0, 180)), # zuf채llige Drehung
    #v2.ColorJitter(brightness=.2, contrast= .2, saturation=.2, hue=.3), # zuf채llig Helligkeit, Kontrast, S채ttigung und Farbton ver채ndern
    v2.PILToTensor(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Lambda(lambda x: torch.where(x<0.2,1,x)),
])

target_transforms = v2.Compose([
    v2.Resize(size=(256, 306)),
    v2.CenterCrop(size=256), 
    v2.PILToTensor(),
    v2.ToDtype(torch.float32, scale=True),
    v2.Lambda(lambda x: torch.where(x<0.2,1,x))
])



# Parameters
params = {'batch_size': 16, 
          'shuffle': True,
          'num_workers': 0}

# Define data loader
train_loader = torch.utils.data.DataLoader(dataset=train_set, **params)

import torch.nn as nn

class Encoder(nn.Module):
    def __init__(self, latent_size, nf, alpha, lernable_bias, dropout):
        super(Encoder, self).__init__()
        
        self.nf = nf
        self.channels = 1 # No. of. Image channels
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(self.channels,self.nf,kernel_size=3, stride=2, padding=1, bias=lernable_bias), 
            nn.LeakyReLU(negative_slope=alpha, inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(self.nf,self.nf*2, kernel_size=3, stride=2, padding=1, bias=lernable_bias),
            nn.BatchNorm2d(num_features=self.nf*2),
            nn.LeakyReLU(negative_slope=alpha, inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(self.nf*2,self.nf*4, kernel_size=5, stride=2, padding=2, bias=lernable_bias),
            nn.BatchNorm2d(num_features=self.nf*4),
            nn.LeakyReLU(negative_slope=alpha, inplace=True)
        )
        self.conv4 = nn.Sequential(
            nn.Conv2d(self.nf*4,self.nf*8, kernel_size=7, stride=3, padding=1, bias=lernable_bias),
            nn.BatchNorm2d(num_features=self.nf*8),
            nn.LeakyReLU(negative_slope=alpha, inplace=True)
        )
        self.conv5 = nn.Sequential(
            nn.Conv2d(self.nf*8,self.nf*16, kernel_size=5, stride=2, padding=0, bias=lernable_bias),
            nn.BatchNorm2d(num_features=self.nf*16),
            nn.LeakyReLU(negative_slope=alpha, inplace=True)
        )
        self.fc = nn.Sequential(
            nn.Linear(self.nf*16*3*3,latent_size), 
            nn.Dropout(p=dropout)
        )
        
    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = x.view(x.size(0),-1)
        x = self.fc(x)
        return x


class Decoder(nn.Module):
    def __init__(self, latent_size, nf, lernable_bias=True):
        super(Decoder, self).__init__()
        
        self.nf=nf
        self.channels = 1 # No. of. Image channels
        
        self.fc = nn.Sequential(
            nn.Linear(latent_size,nf*16*3*3),
            nn.ReLU(True)
        )
        self.convT1 = nn.Sequential(
            nn.ConvTranspose2d(nf*16, nf*8, kernel_size=5, stride=2, padding=0, output_padding=1, bias=lernable_bias),
            nn.BatchNorm2d(num_features=nf*8),
            nn.ReLU(True)
        )
        self.convT2 = nn.Sequential(
            nn.ConvTranspose2d(nf*8, nf*4, kernel_size=7, stride=3, padding=1, output_padding=0, bias=lernable_bias),
            nn.BatchNorm2d(num_features=nf*4),
            nn.ReLU(True)
        )
        self.convT3 = nn.Sequential(
            nn.ConvTranspose2d(nf*4, nf*2, kernel_size=5, stride=2, padding=2, output_padding=1, bias=lernable_bias),
            nn.BatchNorm2d(num_features=nf*2),
            nn.ReLU(True)
        )
        self.convT4 = nn.Sequential(
            nn.ConvTranspose2d(nf*2, nf, kernel_size=3, stride=2, padding=1, output_padding=1, bias=lernable_bias),
            nn.BatchNorm2d(num_features=nf),
            nn.ReLU(True)
        )
        self.convT5 = nn.Sequential(
            nn.ConvTranspose2d(nf, self.channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=lernable_bias),
            #nn.Tanh() 
            nn.Sigmoid()
        )        
        
        
    def forward(self, x):
        x = self.fc(x)
        x = x.reshape(-1,self.nf*16,3,3) 
        x = self.convT1(x)
        x = self.convT2(x)
        x = self.convT3(x)
        x = self.convT4(x)
        x = self.convT5(x)
        return x


class AE(nn.Module):
    def __init__(self, latent_size=1000, nf=32, alpha=0.1, lernable_bias=True, dropout=0.1):
        super(AE, self).__init__()
        self.encoder = Encoder(latent_size, nf, alpha, lernable_bias, dropout)
        self.decoder = Decoder(latent_size, nf, lernable_bias)

    def forward(self, x):
        encode = self.encoder(x)
        decode = self.decoder(encode)
        return decode



# load model in eval mode
model = AE(latent_size=768, nf=48, alpha=.19, lernable_bias=False, dropout=.3).to(device)
checkpoint = torch.load(r'D:\AD\runs\Pernat_final_22-11-2023_105625\Pernat_final_290.pth', map_location=torch.device(device))

#model.load_state_dict(checkpoint)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()



def eval_model(model, criterion, data_loader):

    model.to(device)
    
    # Verluste je Label
    clean_dist = []
    anom_dist = []
     
    # Validation of the model
    model.eval()
    with torch.no_grad():
        # Datensatz
        for image, label in tqdm(data_loader):
            image, label = image.to(device), label.to(device)
            sample = model(image)
            loss = criterion(sample, image)
            if label == 1: # anomaly
                anom_dist.append(loss.item())
            else:
                clean_dist.append(loss.item())
            #print(f'[DEBUG] Label:{label.item()}, Loss:{loss}')
              
    return clean_dist, anom_dist





